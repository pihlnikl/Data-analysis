{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pihlnikl/Data-analysis/blob/master/NLP/basic_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbdnU7HznK0m"
      },
      "source": [
        "# Basic NLP\n",
        "  * Reading and processing language data\n",
        "  * Segmenting text\n",
        "  * Calculating word frequencies and idf weights\n",
        "\n",
        "* Exercises are based on tweets downloaded using Twitter API. Both Finnish and English tweets are available, you are free to choose which language you want to work with.\n",
        "\n",
        "> Finnish: http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\n",
        "\n",
        "> English: http://dl.turkunlp.org/intro-to-nlp/english-tweets-sample.jsonl.gz\n",
        "\n",
        "* Both files include 10,000 tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmlbS9GLm596"
      },
      "source": [
        "## 1) Read tweets in Python\n",
        "\n",
        "* Download the file, and read the data in Python\n",
        "* **The outcome** is a list of tweets, where each tweet is a dictionary including different (key, value) pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F1CZr0f4m5-E"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import gzip\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "# download and read data\n",
        "data = urllib.request.urlopen(\"http://dl.turkunlp.org/intro-to-nlp/english-tweets-sample.jsonl.gz\")\n",
        "with gzip.open(data, 'rb') as f:\n",
        "  res = [json.loads(jline) for jline in f.read().splitlines()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8mHlc_9m5-H"
      },
      "source": [
        "## 2) Extract texts from the tweet jsons\n",
        "\n",
        "* Extract the actual text field for each tweet.\n",
        "* **The outcome** is a list of tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H1hCGcbcm5-J"
      },
      "outputs": [],
      "source": [
        "tweets = []\n",
        "# Extract text from each line\n",
        "for i in res:\n",
        "  tweets.append(i[\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04OtJSi8m5-O"
      },
      "source": [
        "## 3) Segment tweets\n",
        "\n",
        "* Segment tweets using the UDPipe machine learned model.\n",
        "\n",
        "> English model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
        "\n",
        "> Finnish model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/fi.segmenter.udpipe\n",
        "\n",
        "\n",
        "* **The output** is a list of segmented tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRmL8lThm5-P",
        "outputId": "d1100465-171a-4ddc-e2a3-b90ab564efed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File â€˜en.segmenter.udpipeâ€™ already there; not retrieving.\n",
            "\n",
            "Requirement already satisfied: ufal.udpipe in /usr/local/lib/python3.7/dist-packages (1.2.0.3)\n",
            "['Check out my class in # GranblueFantasy !\\nhttps://t.co/pAvXn8diJr\\n', 'Extending a big Thank\\nYou to our Community Partner all over the world !\\nhttps://t.co/cu7on7g1si\\n', 'Blueberry ðŸ¨ https://t.co/2gzHAFWYJY\\n', 'RT @ LILUZIVERT :\\nBad day â˜¹ï¸Â®ï¸\\n', \"@prologve_ @BTS_ARMY @BTS_twt I 'm Chim tho\\n\", 'i need a dog to cuddle with right now\\n', 'RT : Country Inn countryinns # CampSprings ðŸ¨ ðŸ‘‰ðŸš– For Taxi ðŸ“ž703-445-4450\\nhttps://t.co/lXdFUm4qUb\\n', 'RT @ KimKardashian : DAY\\n10 - PENELOPE https://t.co/1z1cgzvZxh\\n', \"RT @ CBCNews :\\nWinnipeggers wake up to the city 's coldest Christmas in 2 decades https://t.co/6R0nw7xDlL https://t.co/k5UuSf3kja\\n\", 'RT @ LuvTyagiTeam : 1id 1vote for 1 episode count hoga\\n']\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "!wget -nc https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
        "!pip3 install ufal.udpipe\n",
        "import ufal.udpipe as udpipe\n",
        "\n",
        "# Define model and pipeline\n",
        "model = udpipe.Model.load(\"en.segmenter.udpipe\")\n",
        "pipeline = udpipe.Pipeline(model, \"tokenize\", \"none\", \"none\", \"horizontal\")\n",
        "\n",
        "# Apply model to each line and append to a list\n",
        "seg_doc = []\n",
        "for i in range(0,len(tweets)):\n",
        "  seg_doc.append(pipeline.process(tweets[i]))\n",
        "\n",
        "# Print to check output\n",
        "print(seg_doc[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgFEubVym5-S"
      },
      "source": [
        "## 4) Calculate word frequencies\n",
        "\n",
        "* Calculate a word frequency list (how many times each word appears) based on the tweets. \n",
        "* Calculate the size of the vocabulary (how many unique words there are).\n",
        "* **The output** is a sorted list of X most common words and their frequencies, and the number of unique words in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L-qLoLum5-S",
        "outputId": "0db323b8-cec0-4d9a-8aa2-8060d969f1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Most common tokens: [('RT', 5744), ('the', 2870), ('to', 2538), ('a', 2221), ('I', 1984), ('and', 1853), ('you', 1786), ('of', 1486), ('for', 1439), ('is', 1415), ('in', 1370), ('on', 923), ('it', 910), ('this', 822), ('that', 785), (\"'s\", 764), ('my', 752), ('with', 663), ('\"', 619), ('your', 614)]\n",
            "Unique words: 35164\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initiate counter\n",
        "token_counter = Counter()\n",
        "# Loop through each line and split words, adding to the counter as we go\n",
        "for i in range(0,len(seg_doc)):\n",
        "  tokens = seg_doc[i].split()\n",
        "  token_counter.update(tokens)\n",
        "\n",
        "# Remove any punctuations and symbols\n",
        "punct = '. , : ( ) ! ? = & - ; ... \\\\ # @ â€¦ '.split() \n",
        "for word, count in list(token_counter.items()):\n",
        "  if word.lower() in punct:\n",
        "    del (token_counter[word])\n",
        "\n",
        "\n",
        "print(\"Most common tokens:\", token_counter.most_common(20))\n",
        "print(\"Unique words:\", len(token_counter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaC9e4Rjm5-T"
      },
      "source": [
        "## 5) Calculate idf weights\n",
        "\n",
        "* Calculate idf weight for each word appearing in the data (one tweet = one document), and print top 20 words with lowest and highest idf values.\n",
        "* **The output** is a list of words sorted by their idf weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pjNY3H9m5-U",
        "outputId": "04639d47-92be-4d1c-80d0-69df6038c3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest idf: [('https://t.co/pAvXn8diJr', 4.0), ('Extending', 4.0), ('Partner', 4.0), ('https://t.co/cu7on7g1si', 4.0), ('https://t.co/2gzHAFWYJY', 4.0), ('Blueberry', 4.0), ('ðŸ¨', 4.0), ('@BTS_ARMY', 4.0), ('@prologve_', 4.0), ('Chim', 4.0), ('cuddle', 4.0), ('countryinns', 4.0), ('https://t.co/lXdFUm4qUb', 4.0), ('CampSprings', 4.0), ('Inn', 4.0), ('https://t.co/1z1cgzvZxh', 4.0), ('PENELOPE', 4.0), ('coldest', 4.0), ('https://t.co/k5UuSf3kja', 4.0), ('https://t.co/6R0nw7xDlL', 4.0)]\n",
            "Lowest idf: [('it', 1.090979145788844), ('-', 1.0888423912600234), ('on', 1.0665127121512945), ('!', 1.0065637695023884), ('in', 0.9118639112994488), ('is', 0.8992849134269184), ('of', 0.8794260687941502), ('for', 0.8738685927380156), ('you', 0.8520146793161949), ('I', 0.8193007987039653), ('#', 0.8181564120552274), ('and', 0.7804154737857453), ('a', 0.7228493860362033), (',', 0.6880343396316336), ('to', 0.6712127996454653), ('the', 0.6476245049994801), ('â€¦', 0.5848596478041272), ('.', 0.5816987086802545), ('RT', 0.24764419584649924), (':', 0.2426039712069758)]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "# Number of tweets\n",
        "m = len(seg_doc)\n",
        "# New counter\n",
        "# In how many tweets each word is present\n",
        "df_counter = Counter()\n",
        "\n",
        "# Loop through each line, split by word and use set to only get unique words per line\n",
        "for i in range(0, len(seg_doc)):\n",
        "  occurrences = set(seg_doc[i].split())\n",
        "  df_counter.update(occurrences)\n",
        "# Apply the df -> idf calculation to each word\n",
        "for word, value in df_counter.items():\n",
        "  df_counter[word] = math.log10(m/float(value))\n",
        "\n",
        "# Highest and lowest idf values\n",
        "print(\"Highest idf:\", df_counter.most_common(20))\n",
        "print(\"Lowest idf:\", df_counter.most_common()[-21:-1])\n",
        "\n",
        "# Note: TF might not have a big impact on its own when processing tweets is because it often lacks\n",
        "# context. If for example someone is doing a sentiment analysis about something, TF could make a term appear popular\n",
        "# even though most of the tweets could be critizising it. TF also doesn't take into account synonyms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwxJM9Rdm5-U"
      },
      "source": [
        "## 6) Duplicates or near duplicates\n",
        "\n",
        "* Check whether we have duplicate tweets (in terms of text field only) in our dataset. Duplicate tweet means here that the exactly same tweet text appears more than once in our dataset.\n",
        "* Also check whether tweets have additional near-duplicates. Near duplicate means here that tweet text is almost the same in two or more tweets.\n",
        "* **The outcome** is a number of unique tweets in our dataset (with possibly counting also which are the most common duplicates) as well as the number of unique tweets after removing also near duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jt0sZrSm5-V",
        "outputId": "fa843f21-436c-45b8-e5b4-3b37b939b996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tweets: 9017\n",
            "Most common duplicates: [('RT @SlushiiMusic: MIC DROP @BTS_twt https://t.co/5p1CArQuaO https://t.co/GnlvhJoetb', 30), ('RT @Louis_Tomlinson: Thank you so much for all the birthday messages and I hope everyone had a great Christmas ! Loads of love', 22), ('RT @lebaenesepapii: yâ€™all couldâ€™ve just said that a transgender couple have a baby rather than giving me brain damage https://t.co/uVO2jEXLâ€¦', 15), ('RT @dril: my friend the only crypto currency you wanna get your hands on is this: bird seed. There is a lot of birds and they all gotta eat', 14), ('RT @GMA: SO excited for @BTS_twt to perform on @NYRE right here on ABC! #RockinEve https://t.co/QN5A3waARg', 14)]\n",
            "Number of unique tweets after removing near duplicates: 9016\n"
          ]
        }
      ],
      "source": [
        "# Import the stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Counter for each task\n",
        "unique_counter = Counter()\n",
        "duplicate_counter = Counter()\n",
        "true_unique = Counter()\n",
        "\n",
        "# Use set to find only unique tweets\n",
        "unique = set(tweets)\n",
        "unique_counter.update(unique)\n",
        "# Find most common full tweets, which means most common full duplicates\n",
        "duplicate_counter.update(tweets)\n",
        "\n",
        "# Loop through each tweet, split into words, steam each word, return to a new list and keep the original\n",
        "# tweet stucture. stemmed = the original tweet with each word stemmed\n",
        "stemmed = [\" \".join([stemmer.stem(word) for word in sentence.split(\" \")]) for sentence in tweets]\n",
        "# Update counter with the stemmed tweets\n",
        "true_unique.update(stemmed)\n",
        "\n",
        "print(\"Number of unique tweets:\", len(unique_counter))\n",
        "print(\"Most common duplicates:\", duplicate_counter.most_common(5))\n",
        "print(\"Number of unique tweets after removing near duplicates:\", len(true_unique))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "basic_nlp.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}