{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbdnU7HznK0m"
      },
      "source": [
        "# Basic NLP\n",
        "  * Reading and processing language data\n",
        "  * Segmenting text\n",
        "  * Calculating word frequencies and idf weights\n",
        "\n",
        "* Exercises are based on tweets downloaded using Twitter API. Both Finnish and English tweets are available, you are free to choose which language you want to work with.\n",
        "\n",
        "> Finnish: http://dl.turkunlp.org/intro-to-nlp/finnish-tweets-sample.jsonl.gz\n",
        "\n",
        "> English: http://dl.turkunlp.org/intro-to-nlp/english-tweets-sample.jsonl.gz\n",
        "\n",
        "* Both files include 10,000 tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmlbS9GLm596"
      },
      "source": [
        "## 1) Read tweets in Python\n",
        "\n",
        "* Download the file, and read the data in Python\n",
        "* **The outcome** is a list of tweets, where each tweet is a dictionary including different (key, value) pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "F1CZr0f4m5-E"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import gzip\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "# download and read data\n",
        "data = urllib.request.urlopen(\"http://dl.turkunlp.org/intro-to-nlp/english-tweets-sample.jsonl.gz\")\n",
        "with gzip.open(data, 'rb') as f:\n",
        "  res = [json.loads(jline) for jline in f.read().splitlines()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8mHlc_9m5-H"
      },
      "source": [
        "## 2) Extract texts from the tweet jsons\n",
        "\n",
        "* Extract the actual text field for each tweet.\n",
        "* **The outcome** is a list of tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "H1hCGcbcm5-J"
      },
      "outputs": [],
      "source": [
        "tweets = []\n",
        "# Extract text from each line\n",
        "for i in res:\n",
        "  tweets.append(i[\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04OtJSi8m5-O"
      },
      "source": [
        "## 3) Segment tweets\n",
        "\n",
        "* Segment tweets using the UDPipe machine learned model.\n",
        "\n",
        "> English model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
        "\n",
        "> Finnish model: https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/fi.segmenter.udpipe\n",
        "\n",
        "\n",
        "* **The output** is a list of segmented tweets, where each tweet is a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRmL8lThm5-P",
        "outputId": "be1e59c6-53bc-4146-a721-ef8d71380af1"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "!wget -nc https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/en.segmenter.udpipe\n",
        "!pip3 install ufal.udpipe\n",
        "import ufal.udpipe as udpipe\n",
        "\n",
        "# Define model and pipeline\n",
        "model = udpipe.Model.load(\"en.segmenter.udpipe\")\n",
        "pipeline = udpipe.Pipeline(model, \"tokenize\", \"none\", \"none\", \"horizontal\")\n",
        "\n",
        "# Apply model to each line and append to a list\n",
        "seg_doc = []\n",
        "for i in range(0,len(tweets)):\n",
        "  seg_doc.append(pipeline.process(tweets[i]))\n",
        "\n",
        "# Print to check output\n",
        "print(seg_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgFEubVym5-S"
      },
      "source": [
        "## 4) Calculate word frequencies\n",
        "\n",
        "* Calculate a word frequency list (how many times each word appears) based on the tweets. \n",
        "* Calculate the size of the vocabulary (how many unique words there are).\n",
        "* **The output** is a sorted list of X most common words and their frequencies, and the number of unique words in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L-qLoLum5-S",
        "outputId": "2b5aea16-2304-40fb-ab1b-93b1465acf96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Most common tokens: [('@', 7150), (':', 6589), ('RT', 5744), ('.', 3518), ('#', 2897), ('the', 2870), (',', 2863), ('…', 2614), ('to', 2538), ('a', 2221), ('I', 1984), ('and', 1853), ('you', 1786), ('of', 1486), ('for', 1439), ('is', 1415), ('in', 1370), ('-', 1171), ('!', 1161), ('on', 923)]\n",
            "Unique words: 35180\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initiate counter\n",
        "token_counter = Counter()\n",
        "# Loop through each line and split words, adding to the counter as we go\n",
        "for i in range(0,len(seg_doc)):\n",
        "  tokens = seg_doc[i].split()\n",
        "  token_counter.update(tokens)\n",
        "\n",
        "#filtered = []\n",
        "#punct = '. , : ( ) ! ? \" = & - ; ... \\\\ '.split() \n",
        "#for word, count in token_counter.most_common(100):\n",
        "#  if word.lower() in stopwords.words(\"english\") or word in punct:\n",
        "#    continue\n",
        "#  filtered.append((word, count))\n",
        "\n",
        "\n",
        "print(\"Most common tokens:\", token_counter.most_common(20))\n",
        "print(\"Unique words:\", len(token_counter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaC9e4Rjm5-T"
      },
      "source": [
        "## 5) Calculate idf weights\n",
        "\n",
        "* Calculate idf weight for each word appearing in the data (one tweet = one document), and print top 20 words with lowest and highest idf values.\n",
        "* Can you think of a reason why someone could claim that tf does not have a high impact when processing tweets?\n",
        "* **The output of this excercise** should be a list of words sorted by their idf weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pjNY3H9m5-U",
        "outputId": "34a4f5fc-7b8d-49e2-cd38-f4182be57c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest idf: [('https://t.co/pAvXn8diJr', 4.0), ('Partner', 4.0), ('Extending', 4.0), ('https://t.co/cu7on7g1si', 4.0), ('Blueberry', 4.0), ('🍨', 4.0), ('https://t.co/2gzHAFWYJY', 4.0), ('Chim', 4.0), ('@prologve_', 4.0), ('@BTS_ARMY', 4.0), ('cuddle', 4.0), ('Inn', 4.0), ('https://t.co/lXdFUm4qUb', 4.0), ('countryinns', 4.0), ('CampSprings', 4.0), ('PENELOPE', 4.0), ('https://t.co/1z1cgzvZxh', 4.0), ('CBCNews', 4.0), ('https://t.co/6R0nw7xDlL', 4.0), ('coldest', 4.0)]\n",
            "Lowest idf: [('it', 1.090979145788844), ('-', 1.0888423912600234), ('on', 1.0665127121512945), ('!', 1.0065637695023884), ('in', 0.9118639112994488), ('is', 0.8992849134269184), ('of', 0.8794260687941502), ('for', 0.8738685927380156), ('you', 0.8520146793161949), ('I', 0.8193007987039653), ('#', 0.8181564120552274), ('and', 0.7804154737857453), ('a', 0.7228493860362033), (',', 0.6880343396316336), ('to', 0.6712127996454653), ('the', 0.6476245049994801), ('…', 0.5848596478041272), ('.', 0.5816987086802545), ('RT', 0.24764419584649924), (':', 0.2426039712069758)]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "# Number of tweets\n",
        "m = len(seg_doc)\n",
        "# New counter\n",
        "# In how many tweets each word is present\n",
        "df_counter = Counter()\n",
        "\n",
        "# Loop through each line, split by word and use set to only get unique words per line\n",
        "for i in range(0, len(seg_doc)):\n",
        "  occurrences = set(seg_doc[i].split())\n",
        "  df_counter.update(occurrences)\n",
        "# Apply the df -> idf calculation to each word\n",
        "for word, value in df_counter.items():\n",
        "  df_counter[word] = math.log10(m/float(value))\n",
        "\n",
        "# Highest and lowest idf values\n",
        "print(\"Highest idf:\", df_counter.most_common(20))\n",
        "print(\"Lowest idf:\", df_counter.most_common()[-21:-1])\n",
        "\n",
        "# TF might not have a big impact on its own when processing tweets is because it often lacks\n",
        "# context. If for example someone is doing a sentiment analysis about something, TF could make a term appear popular\n",
        "# even though most of the tweets could be critizising it. TF also doesn't take into account synonyms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwxJM9Rdm5-U"
      },
      "source": [
        "## 6) Duplicates or near duplicates\n",
        "\n",
        "* Check whether we have duplicate tweets (in terms of text field only) in our dataset. Duplicate tweet means here that the exactly same tweet text appears more than once in our dataset.\n",
        "* Note: It makes sense to check the duplicates using original tweet texts as the texts were before segmentation. I would also recommend using the full 10,000 dataset here in order to get higher chance of seeing duplicates (this does not require heavy computing).\n",
        "* Try to check whether tweets have additional near-duplicates. Near duplicate means here that tweet text is almost the same in two or more tweets. Ponder what kind of near duplicates there could be and how to find those. Start by considering for example different normalization techniques. Implement some of the techniques you considered.\n",
        "* **The outcome of this exercise** should be a number of unique tweets in our dataset (with possibly counting also which are the most common duplicates) as well as the number of unique tweets after removing also near duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jt0sZrSm5-V",
        "outputId": "6782df7e-330b-444f-b2c1-bbb0495529c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique tweets: 9017\n",
            "Most common duplicates: [('RT @SlushiiMusic: MIC DROP @BTS_twt https://t.co/5p1CArQuaO https://t.co/GnlvhJoetb', 30), ('RT @Louis_Tomlinson: Thank you so much for all the birthday messages and I hope everyone had a great Christmas ! Loads of love', 22), ('RT @lebaenesepapii: y’all could’ve just said that a transgender couple have a baby rather than giving me brain damage https://t.co/uVO2jEXL…', 15), ('RT @dril: my friend the only crypto currency you wanna get your hands on is this: bird seed. There is a lot of birds and they all gotta eat', 14), ('RT @GMA: SO excited for @BTS_twt to perform on @NYRE right here on ABC! #RockinEve https://t.co/QN5A3waARg', 14)]\n",
            "Number of unique tweets after removing near duplicates: 9016\n"
          ]
        }
      ],
      "source": [
        "# Import the stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Counter for each task\n",
        "unique_counter = Counter()\n",
        "duplicate_counter = Counter()\n",
        "true_unique = Counter()\n",
        "\n",
        "# Use set to find only unique tweets\n",
        "unique = set(tweets)\n",
        "unique_counter.update(unique)\n",
        "# Find most common full tweets, which means most common full duplicates\n",
        "duplicate_counter.update(tweets)\n",
        "\n",
        "# Loop through each tweet, split into words, steam each word, return to a new list and keep the original\n",
        "# tweet stucture. stemmed = the original tweet with each word stemmed\n",
        "stemmed = [\" \".join([stemmer.stem(word) for word in sentence.split(\" \")]) for sentence in tweets]\n",
        "# Update counter with the stemmed tweets\n",
        "true_unique.update(stemmed)\n",
        "\n",
        "print(\"Number of unique tweets:\", len(unique_counter))\n",
        "print(\"Most common duplicates:\", duplicate_counter.most_common(5))\n",
        "print(\"Number of unique tweets after removing near duplicates:\", len(true_unique))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_1_pihlnikl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
