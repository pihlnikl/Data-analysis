{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "sentiment_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pihlnikl/Data-analysis/blob/master/sentiment_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjIThqNW9ZAU"
      },
      "source": [
        "# IMDb sentiment classifier\n",
        "Comparing results of using CountVectorizer vs TfidfVectorizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxbv7By-57T4",
        "outputId": "7518eb48-2f04-4a51-8210-62a64ff3f52b"
      },
      "source": [
        "# Download the data\n",
        "!wget -nc https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/imdb_train.json\n",
        "!wget -nc http://dl.turkunlp.org/intro-to-nlp.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-20 15:31:08--  https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/imdb_train.json\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/TurkuNLP/intro-to-nlp/master/Data/imdb_train.json [following]\n",
            "--2021-12-20 15:31:08--  https://raw.githubusercontent.com/TurkuNLP/intro-to-nlp/master/Data/imdb_train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33944099 (32M) [text/plain]\n",
            "Saving to: ‘imdb_train.json’\n",
            "\n",
            "imdb_train.json     100%[===================>]  32.37M   144MB/s    in 0.2s    \n",
            "\n",
            "2021-12-20 15:31:09 (144 MB/s) - ‘imdb_train.json’ saved [33944099/33944099]\n",
            "\n",
            "--2021-12-20 15:31:09--  http://dl.turkunlp.org/intro-to-nlp.tar.gz\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76192539 (73M) [application/octet-stream]\n",
            "Saving to: ‘intro-to-nlp.tar.gz’\n",
            "\n",
            "intro-to-nlp.tar.gz 100%[===================>]  72.66M  19.3MB/s    in 4.7s    \n",
            "\n",
            "2021-12-20 15:31:14 (15.5 MB/s) - ‘intro-to-nlp.tar.gz’ saved [76192539/76192539]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXG8Ca0e5hKz"
      },
      "source": [
        "# Import packages\n",
        "import json\n",
        "import random\n",
        "with open(\"imdb_train.json\") as f:\n",
        "    data = json.load(f)\n",
        "random.shuffle(data)\n",
        "\n",
        "# Split data into labels and text\n",
        "texts = [one_example[\"text\"] for one_example in data]\n",
        "labels = [one_example[\"class\"] for one_example in data]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amrLFpve5hK0",
        "outputId": "9f1e8dbf-ad25-4d19-c6ee-7e8cc44efb03"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Import TfidVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into 2 different train & test sets for comparison\n",
        "train_texts, dev_texts, train_labels, dev_labels = train_test_split(texts,labels,test_size=0.2)\n",
        "train_texts_T, dev_texts_T, train_labels_T, dev_labels_T = train_test_split(texts,labels,test_size=0.2)\n",
        "\n",
        "# Define vectorizer using CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=100000,binary=True,ngram_range=(1,2))\n",
        "feature_matrix_train = vectorizer.fit_transform(train_texts)\n",
        "feature_matrix_dev = vectorizer.transform(dev_texts)\n",
        "\n",
        "# Lets compare with TfidVectorizer\n",
        "vectorizer_T = TfidfVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
        "# Same with the new vectorizer\n",
        "feature_matrix_train_T = vectorizer_T.fit_transform(train_texts_T)\n",
        "feature_matrix_dev_T = vectorizer_T.transform(dev_texts_T)\n",
        "\n",
        "print(\"shape =\",feature_matrix_train.shape)\n",
        "print(\"shape =\", feature_matrix_train_T.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape = (20000, 100000)\n",
            "shape = (20000, 68320)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC0Rmp-58jj-"
      },
      "source": [
        "\n",
        "\n",
        "*   We can see that changing the lenght of n-grams already effects the shape of the data, which was previously (20000, 68390)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTsbLacCAAAo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Function for converting matrix to tensor\n",
        "def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "    coo = X.tocoo()\n",
        "    indices = np.mat([coo.row, coo.col]).transpose()\n",
        "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))\n",
        "\n",
        "# Convert basic matrix into tensor\n",
        "feature_matrix_train_tf = convert_sparse_matrix_to_sparse_tensor(feature_matrix_train)\n",
        "feature_matrix_dev_tf = convert_sparse_matrix_to_sparse_tensor(feature_matrix_dev)\n",
        "\n",
        "# Same with TfidVectorizer\n",
        "feature_matrix_train_tf_T = convert_sparse_matrix_to_sparse_tensor(feature_matrix_train_T)\n",
        "feature_matrix_dev_tf_T = convert_sparse_matrix_to_sparse_tensor(feature_matrix_dev_T)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIluAddR5hK6"
      },
      "source": [
        "Now we have the feature matrix done! Next thing we need is the class labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE7xaPp65hK7",
        "outputId": "f4e57097-46e1-4b0c-ac92-48bde65532e9"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define the labels for both datasets\n",
        "label_encoder = LabelEncoder()\n",
        "class_numbers_train = label_encoder.fit_transform(train_labels)\n",
        "class_numbers_dev = label_encoder.transform(dev_labels)\n",
        "\n",
        "# Same procedure with Tfid\n",
        "class_numbers_train_T = label_encoder.fit_transform(train_labels_T)\n",
        "class_numbers_dev_T = label_encoder.transform(dev_labels_T)\n",
        "\n",
        "# Review can be either negative (neg) or positive (pos)\n",
        "print(\"class_numbers shape =\",class_numbers_train.shape)\n",
        "print(\"class labels\",label_encoder.classes_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_numbers shape = (20000,)\n",
            "class labels ['neg' 'pos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLzFHV2d5hK8"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "example_count, feature_count = feature_matrix_train.shape\n",
        "example_count2 = class_numbers_train.shape[0]\n",
        "assert example_count == example_count2\n",
        "class_count = len(label_encoder.classes_)\n",
        "\n",
        "#Build the network:\n",
        "inp = Input(shape=(feature_count,))\n",
        "hidden = Dense(200,activation=\"tanh\")(inp)\n",
        "outp = Dense(class_count,activation=\"softmax\")(hidden)\n",
        "model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "# The same procedures for Tfid\n",
        "example_count_T, feature_count_T = feature_matrix_train_T.shape\n",
        "example_count2_T = class_numbers_train_T.shape[0]\n",
        "assert example_count_T == example_count2_T\n",
        "class_count_T = len(label_encoder.classes_)\n",
        "\n",
        "inp_T = Input(shape=(feature_count_T,))\n",
        "hidden_T = Dense(200,activation=\"tanh\")(inp_T)\n",
        "outp_T = Dense(class_count,activation=\"softmax\")(hidden_T)\n",
        "model_T = Model(inputs=[inp_T], outputs=[outp_T])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2uv181F5hK-"
      },
      "source": [
        "model.compile(optimizer=\"sgd\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
        "model_T.compile(optimizer=\"sgd\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl2XExJk5hK-"
      },
      "source": [
        "A compiled model can be fitted on data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzZbwpRG5hK-",
        "outputId": "56fb12da-dbe6-41b3-d6c8-73b5e735a2f3"
      },
      "source": [
        "hist=model.fit(feature_matrix_train_tf,class_numbers_train,\\\n",
        "               validation_data=(feature_matrix_dev_tf,class_numbers_dev),\\\n",
        "               batch_size=100,verbose=1,epochs=5)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 200), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200/200 [==============================] - 5s 14ms/step - loss: 0.5477 - accuracy: 0.7814 - val_loss: 0.4625 - val_accuracy: 0.8284\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 3s 13ms/step - loss: 0.4039 - accuracy: 0.8571 - val_loss: 0.3879 - val_accuracy: 0.8530\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 3s 13ms/step - loss: 0.3406 - accuracy: 0.8770 - val_loss: 0.3512 - val_accuracy: 0.8610\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 3s 13ms/step - loss: 0.3012 - accuracy: 0.8902 - val_loss: 0.3263 - val_accuracy: 0.8672\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 3s 13ms/step - loss: 0.2720 - accuracy: 0.9021 - val_loss: 0.3110 - val_accuracy: 0.8710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPCp0BZ-lvqM",
        "outputId": "b21b4e21-2766-40f0-d04d-bc588ce09efb"
      },
      "source": [
        "hist_T=model_T.fit(feature_matrix_train_tf_T,class_numbers_train_T,\\\n",
        "               validation_data=(feature_matrix_dev_tf_T,class_numbers_dev_T),\\\n",
        "               batch_size=100,verbose=1,epochs=5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_1/dense_2/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_1/dense_2/embedding_lookup_sparse/Reshape:0\", shape=(None, 200), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_1/dense_2/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200/200 [==============================] - 3s 12ms/step - loss: 0.6925 - accuracy: 0.5366 - val_loss: 0.6918 - val_accuracy: 0.5438\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 2s 11ms/step - loss: 0.6906 - accuracy: 0.5767 - val_loss: 0.6899 - val_accuracy: 0.6326\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 2s 11ms/step - loss: 0.6886 - accuracy: 0.6312 - val_loss: 0.6881 - val_accuracy: 0.7030\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 2s 11ms/step - loss: 0.6867 - accuracy: 0.6646 - val_loss: 0.6861 - val_accuracy: 0.7772\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 2s 11ms/step - loss: 0.6847 - accuracy: 0.7199 - val_loss: 0.6842 - val_accuracy: 0.7906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZZRz1MY9lJB"
      },
      "source": [
        "\n",
        "\n",
        "*   Increasing the ngram_range seems to have hurt the accuracy score\n",
        "\n",
        "*   Accuracy with the original range was in the high 0.9x, where as the increased gives a score in the low 0.9x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbz1dBZI5hK_",
        "outputId": "09033b28-9b23-4b6a-cf32-d9be2931e7d8"
      },
      "source": [
        "print(hist.history[\"val_accuracy\"])\n",
        "print(hist_T.history[\"val_accuracy\"])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8284000158309937, 0.8529999852180481, 0.8610000014305115, 0.8672000169754028, 0.8709999918937683]\n",
            "[0.5437999963760376, 0.6326000094413757, 0.703000009059906, 0.7771999835968018, 0.7906000018119812]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYaG8VY-mZMT"
      },
      "source": [
        "\n",
        "*   As we can see, there are some small, almost minimal differences in the results thus far between CountVectorizer and TfidVectorizer.\n",
        "\n",
        "*   The largest differences seem to be in the loss categories\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtLKUWW45hLA",
        "outputId": "178a9878-7fe2-4756-8687-0146d12eb467"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "def save_model(file_name,model,label_encoder,vectorizer):\n",
        "    \"\"\"Saves model structure and vocabularies\"\"\"\n",
        "    model_json = model.to_json()\n",
        "    with open(file_name+\".model.json\", \"w\") as f:\n",
        "        print(model_json,file=f)\n",
        "    with open(file_name+\".encoders.pickle\",\"wb\") as f:\n",
        "        pickle.dump((label_encoder,vectorizer),f)\n",
        "            \n",
        "# Let's try a different optimizer!\n",
        "opt = tf.optimizers.Adam()\n",
        "model.compile(optimizer = opt, loss = \"sparse_categorical_crossentropy\", metrics = ['accuracy'])\n",
        "model_T.compile(optimizer = opt, loss = \"sparse_categorical_crossentropy\", metrics = ['accuracy'])\n",
        "\n",
        "# Save model and vocabularies, can be done before training\n",
        "os.makedirs(\"models\", exist_ok = True)\n",
        "save_model(\"models/imdb_bow\", model, label_encoder, vectorizer)\n",
        "# Same for Tfid\n",
        "os.makedirs(\"models_T\",exist_ok = True)\n",
        "save_model(\"models_T/imdb_bow\", model_T, label_encoder, vectorizer_T)\n",
        "\n",
        "# Callback function to save weights during training, if validation loss goes down\n",
        "save_cb = ModelCheckpoint(filepath=\"models/imdb_bow.weights.h5\", monitor='val_loss',\\\n",
        "                        verbose=1, save_best_only=True, mode='auto')\n",
        "stop_cb = EarlyStopping(patience=2,verbose=1,restore_best_weights=True)\n",
        "\n",
        "# Same for Tfid\n",
        "save_cb_T = ModelCheckpoint(filepath=\"models_T/imdb_bow.weights.h5\", monitor='val_loss',\\\n",
        "                        verbose=1, save_best_only=True, mode='auto')\n",
        "stop_cb_T = EarlyStopping(patience=2,verbose=1,restore_best_weights=True)\n",
        "\n",
        "hist = model.fit(feature_matrix_train_tf, class_numbers_train,\\\n",
        "               validation_data = (feature_matrix_dev_tf,class_numbers_dev),\\\n",
        "               batch_size = 100,verbose = 1,epochs = 20,\\\n",
        "               callbacks = [save_cb, stop_cb])\n",
        "\n",
        "hist_T = model_T.fit(feature_matrix_train_tf_T, class_numbers_train_T,\\\n",
        "               validation_data = (feature_matrix_dev_tf_T, class_numbers_dev_T),\\\n",
        "               batch_size = 200, verbose = 1, epochs = 20,\\\n",
        "               callbacks = [save_cb_T, stop_cb_T])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 200), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200/200 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.8900\n",
            "Epoch 00001: val_loss improved from inf to 0.25592, saving model to models/imdb_bow.weights.h5\n",
            "200/200 [==============================] - 9s 38ms/step - loss: 0.2802 - accuracy: 0.8900 - val_loss: 0.2559 - val_accuracy: 0.8930\n",
            "Epoch 2/20\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0206 - accuracy: 0.9956\n",
            "Epoch 00002: val_loss did not improve from 0.25592\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0210 - accuracy: 0.9955 - val_loss: 0.3500 - val_accuracy: 0.8896\n",
            "Epoch 3/20\n",
            "199/200 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9998\n",
            "Epoch 00003: val_loss did not improve from 0.25592\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.3927 - val_accuracy: 0.8906\n",
            "Epoch 00003: early stopping\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_1/dense_2/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_1/dense_2/embedding_lookup_sparse/Reshape:0\", shape=(None, 200), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_1/dense_2/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - ETA: 0s - loss: 0.3219 - accuracy: 0.8652\n",
            "Epoch 00001: val_loss improved from inf to 0.26551, saving model to models_T/imdb_bow.weights.h5\n",
            "100/100 [==============================] - 4s 33ms/step - loss: 0.3219 - accuracy: 0.8652 - val_loss: 0.2655 - val_accuracy: 0.8866\n",
            "Epoch 2/20\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9841\n",
            "Epoch 00002: val_loss did not improve from 0.26551\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0553 - accuracy: 0.9841 - val_loss: 0.3212 - val_accuracy: 0.8796\n",
            "Epoch 3/20\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9982\n",
            "Epoch 00003: val_loss did not improve from 0.26551\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0133 - accuracy: 0.9982 - val_loss: 0.3710 - val_accuracy: 0.8770\n",
            "Epoch 00003: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va1qwl681saR"
      },
      "source": [
        "\n",
        "\n",
        "*   Again, we can see that there is some difference in the results, although not large.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBKFuz2z5hLC",
        "outputId": "cf316160-07f5-4869-f9d3-8481f9d784e4"
      },
      "source": [
        "import numpy\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#Validation data used during training:\n",
        "val_instances, val_labels = feature_matrix_dev_tf, class_numbers_dev\n",
        "\n",
        "# Same for Tfid\n",
        "val_instances_T, val_labels_T = feature_matrix_dev_tf_T, class_numbers_dev_T\n",
        "\n",
        "print(\"Network output=\",model.predict(val_instances))\n",
        "predictions=numpy.argmax(model.predict(val_instances),axis=1)\n",
        "print(\"Maximum class for each example=\",predictions)\n",
        "gold=val_labels\n",
        "gold_T=val_labels_T\n",
        "conf_matrix=confusion_matrix(list(gold),list(predictions))\n",
        "print(\"Confusion matrix=\\n\",conf_matrix)\n",
        "\n",
        "predictions_T = numpy.argmax(model_T.predict(val_instances_T),axis=1)\n",
        "conf_matrix_T = confusion_matrix(list(gold_T),list(predictions_T))\n",
        "print(\"Confusion matrix Tfid=\\n\", conf_matrix_T)\n",
        "\n",
        "gold_labels=label_encoder.inverse_transform(list(gold))\n",
        "gold_labels_T=label_encoder.inverse_transform(list(gold_T))\n",
        "\n",
        "predicted_labels=label_encoder.inverse_transform(list(predictions))\n",
        "predicted_labels_T=label_encoder.inverse_transform(list(predictions_T))\n",
        "\n",
        "print(classification_report(gold_labels,predicted_labels))\n",
        "print(classification_report(gold_labels_T,predicted_labels_T))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network output= [[7.4333906e-01 2.5666091e-01]\n",
            " [1.6530928e-03 9.9834692e-01]\n",
            " [9.0480202e-01 9.5197946e-02]\n",
            " ...\n",
            " [5.8537900e-01 4.1462100e-01]\n",
            " [7.1745858e-02 9.2825419e-01]\n",
            " [9.9900621e-01 9.9386845e-04]]\n",
            "Maximum class for each example= [0 1 0 ... 0 1 0]\n",
            "Confusion matrix=\n",
            " [[2275  248]\n",
            " [ 287 2190]]\n",
            "Confusion matrix Tfid=\n",
            " [[2172  258]\n",
            " [ 309 2261]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.89      0.90      0.89      2523\n",
            "         pos       0.90      0.88      0.89      2477\n",
            "\n",
            "    accuracy                           0.89      5000\n",
            "   macro avg       0.89      0.89      0.89      5000\n",
            "weighted avg       0.89      0.89      0.89      5000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.88      0.89      0.88      2430\n",
            "         pos       0.90      0.88      0.89      2570\n",
            "\n",
            "    accuracy                           0.89      5000\n",
            "   macro avg       0.89      0.89      0.89      5000\n",
            "weighted avg       0.89      0.89      0.89      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-I6zImsv8x-"
      },
      "source": [
        "\n",
        "\n",
        "*   The differences between CountVectorizer and Tfid are at this point so small that the classification reports look almost identical\n",
        "\n",
        "\n"
      ]
    }
  ]
}